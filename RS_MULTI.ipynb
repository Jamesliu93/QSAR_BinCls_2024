{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Basic libraries ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Feature generation ===\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.SaltRemover import SaltRemover\n",
    "from mordred import Calculator, descriptors\n",
    "\n",
    "# === Impute, oversample, and undersample ===\n",
    "from sklearn.impute import SimpleImputer\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# === Dimensionality reduction ===\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# === Classifiers ===\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier, DMatrix, train\n",
    "\n",
    "# === Metrics and cross-validation ====\n",
    "from sklearn import metrics as met\n",
    "from sklearn.model_selection import cross_val_predict, LeaveOneOut, KFold\n",
    "import shap\n",
    "\n",
    "# === Neural networks ===\n",
    "import tensorflow as tf\n",
    "from keras import Model, layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.data import Iterator\n",
    "\n",
    "# === Neural networks from Pytorch Lightning ===\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "\n",
    "# === Optimization ===\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb88f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Read in files with X,Y ===\n",
    "RS_XY = pd.read_csv('rsxy_v1.csv')\n",
    "clist = list(RS_XY['SMILES'])\n",
    "y = np.array(RS_XY['Sens'])\n",
    "y = np.reshape(y,(-1,1))\n",
    "X = np.zeros(shape=(len(clist),1826))\n",
    "\n",
    "# === Calculate descriptors ===\n",
    "calc = Calculator(descriptors)\n",
    "dlist = list(calc._name_dict.keys())\n",
    "remover = SaltRemover()\n",
    "for i in range(len(clist)):\n",
    "    mol = Chem.MolFromSmiles(clist[i])\n",
    "    #mol = remover.StripMol(mol)\n",
    "    X[i,:] = calc(mol)\n",
    "sh1 = np.shape(X)\n",
    "print(f'Shape | raw: {sh1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Set up oversampler ===\n",
    "ovs = SMOTE()\n",
    "\n",
    "# === Impute missing values ===\n",
    "imp_med = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "Xi = imp_med.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11caa4b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Filter data and scale ===\n",
    "X0 = X\n",
    "X = X[:,~np.any(np.isnan(X), axis=0)]\n",
    "X = X[:, np.var(X, axis=0) != 0]\n",
    "Xs = pp.MinMaxScaler().fit_transform(X)\n",
    "sh2 = np.shape(Xs)\n",
    "print(f'Shape | filtered/scaled: {sh2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Filter data and scale ===\n",
    "X_pd = pd.DataFrame(X0, columns=dlist)\n",
    "X_pd.to_csv('temp_out/X.csv', header=True)\n",
    "Xs_pd = X_pd.dropna(axis=1)\n",
    "Xs_pd = Xs_pd.loc[:, Xs_pd.var()!=0]\n",
    "Xs_pd.to_csv('temp_out/Xs.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46b0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Filter data and scale ===\n",
    "Xi = Xi[:,~np.any(np.isnan(Xi), axis=0)]\n",
    "Xi = Xi[:, np.var(Xi, axis=0) != 0]\n",
    "Xis = pp.MinMaxScaler().fit_transform(Xi)\n",
    "sh3 = np.shape(Xis)\n",
    "print(f'Shape | filtered/scaled: {sh3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17952878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Heatmap of feature correlation ===\n",
    "Xs_pd = pd.DataFrame(Xs)\n",
    "hm1 = sns.heatmap(Xs_pd.corr())\n",
    "hm1.figure.savefig('temp_out/Xs.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc607a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Imputed features ===\n",
    "Xis_pd = pd.DataFrame(Xis)\n",
    "hm1 = sns.heatmap(Xis_pd.corr())\n",
    "hm1.figure.savefig('temp_out/Xis.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401449b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Conduct PCA and display updated heatmap ===\n",
    "pca = PCA(n_components=100,random_state=np.random.seed(0))\n",
    "Xsr = pca.fit_transform(Xs_pd)\n",
    "Xsr_pd = pd.DataFrame(Xsr)\n",
    "hm2 = sns.heatmap(Xsr_pd.corr())\n",
    "hm2.figure.savefig('temp_out/Xsr.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PCA for imputed features ===\n",
    "Xisr = pca.fit_transform(Xis_pd)\n",
    "Xisr_pd = pd.DataFrame(Xisr)\n",
    "hm2 = sns.heatmap(Xisr_pd.corr())\n",
    "hm2.figure.savefig('temp_out/Xisr.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define creation of artificial neural network ===\n",
    "def create_ann(d,l):\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=(l,)))\n",
    "    model.add(Dropout(d,input_shape=(l,)))\n",
    "    model.add(Dense(50,activation='relu',name='hl_2',kernel_regularizer=tf.keras.regularizers.L1(0)))\n",
    "    model.add(Dense(25,activation='relu',name='hl_3',kernel_regularizer=tf.keras.regularizers.L1(0)))\n",
    "    model.add(Dense(1,activation='linear',name='l_o',kernel_regularizer=tf.keras.regularizers.L1(0)))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f3821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Convert to torch tensors ===\n",
    "def to_tensor(X_trn, y_trn, X_tst, y_tst):\n",
    "    X_trn = torch.tensor(X_trn, dtype=torch.float32)\n",
    "    y_trn = torch.tensor(y_trn, dtype=torch.int32).reshape(-1, 1)\n",
    "    X_tst = torch.tensor(X_tst, dtype=torch.float32)\n",
    "    y_tst = torch.tensor(y_tst, dtype=torch.int32).reshape(-1, 1)\n",
    "    return X_trn, y_trn, X_tst, y_tst\n",
    "\n",
    "class rscls(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(x.shape[0], 50)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(50, 25)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.out = nn.Linear(25, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.a1(self.l1(x))\n",
    "        x = self.a2(self.l2(x))\n",
    "        x = self.sig(self.out(x))\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward()\n",
    "        loss = nn.BCEWithLogitsLoss(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward()\n",
    "        loss = self.cel(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = mse_loss(logits, y)\n",
    "        correct = torch.sum(logits == y.data)\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        logs = {'test_loss': avg_loss}      \n",
    "        return {'avg_test_loss': avg_loss, 'log': logs, 'progress_bar': logs }\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TensorDataset(torch.tensor(train_features.values).float(), torch.tensor(train_targets[['cnt']].values).float())\n",
    "        train_loader = DataLoader(dataset = train_dataset, batch_size = 128)\n",
    "        return train_loader\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        validation_dataset = TensorDataset(torch.tensor(validation_features.values).float(), torch.tensor(validation_targets[['cnt']].values).float())\n",
    "        validation_loader = DataLoader(dataset = validation_dataset, batch_size = 128)\n",
    "        return validation_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TensorDataset(torch.tensor(test_features.values).float(), torch.tensor(test_targets[['cnt']].values).float())\n",
    "        test_loader = DataLoader(dataset = test_dataset, batch_size = 128)\n",
    "        return test_loader\n",
    "        \n",
    "    \n",
    "    \n",
    "#class qsar_dmod(pl.LightningDataModule):\n",
    "        \n",
    "\n",
    "# trainer = pl.Trainer()\n",
    "# trainer.fit(AAAAAA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#model = rscls()\n",
    "#trainer = pl.Trainer(max_epochs=50,\n",
    "#                    progress_bar_refresh_rate=20)\n",
    "#trainer.fit(model,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cross validation function ===\n",
    "def qsar_cv(X, y, n, m, p1, p2, p3, nspl=5, oversample='F'):\n",
    "    # === Initialize metrtics and iterator ===\n",
    "    acc, pre, rec, f1s = (np.zeros(nspl*n) for i in range(4))\n",
    "    i = 0\n",
    "    # === Start stopwatch ===\n",
    "    t_sta = time.perf_counter()\n",
    "    \n",
    "    # === Loop over n random seeds ===\n",
    "    ytst_cv = []\n",
    "    prob_cv = []\n",
    "    for rs in range(n):\n",
    "        # === Initialize k-fold cross validation ===\n",
    "        kfold = KFold(n_splits=nspl, shuffle=True, random_state=np.random.seed(n))\n",
    "        # === Set model ===\n",
    "        if m == 'LR':\n",
    "            model = LogisticRegression(solver=p1,random_state=np.random.seed(i),max_iter=200)\n",
    "        if m == 'SVM':\n",
    "            model = svm.SVC(kernel=p1,random_state=np.random.seed(i),probability=True)\n",
    "        if m == 'RF':\n",
    "            model = RandomForestClassifier(n_estimators=p1,max_depth=p2,random_state=np.random.seed(i))\n",
    "        if m == 'GBT':\n",
    "            model = XGBClassifier(max_depth=p1,seed=i)\n",
    "        \n",
    "        for trn_i, tst_i in kfold.split(X):\n",
    "            # === Split data ===\n",
    "            X_trn, X_tst = X[trn_i], X[tst_i]\n",
    "            y_trn, y_tst = y[trn_i], y[tst_i]\n",
    "            # === Oversample ===\n",
    "            if oversample == 'T':\n",
    "                X_trn, y_trn = ovs.fit_resample(X_trn, y_trn)\n",
    "            \n",
    "            # === Fit and predict ===\n",
    "            model.fit(X_trn, y_trn.ravel())\n",
    "            prob = model.predict_proba(X_tst)[:,1]\n",
    "            pred = model.predict(X_tst)\n",
    "            \n",
    "            # === Append for performance curves ===\n",
    "            ytst_cv.append(y_tst)\n",
    "            prob_cv.append(prob)\n",
    "            \n",
    "            # === Calculate metrics ===\n",
    "            acc[i] = met.accuracy_score(y_tst,pred)\n",
    "            pre[i] = met.precision_score(y_tst,pred)\n",
    "            rec[i] = met.recall_score(y_tst,pred)\n",
    "            f1s[i] = met.f1_score(y_tst,pred)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "    # === Stop stopwatch ===\n",
    "    t_end = time.perf_counter()\n",
    "    t_ela = t_end-t_sta\n",
    "\n",
    "    # === Average metrics ===\n",
    "    m_acc = np.mean(acc)\n",
    "    m_pre = np.mean(pre)\n",
    "    m_rec = np.mean(rec)\n",
    "    m_f1s = np.mean(f1s)\n",
    "    ytst_cv = np.concatenate(ytst_cv)\n",
    "    prob_cv = np.concatenate(prob_cv)\n",
    "    prc = met.precision_recall_curve(ytst_cv, prob_cv)\n",
    "    roc = met.roc_curve(ytst_cv, prob_cv)\n",
    "    \n",
    "    ret = {'Model':m, 'Param_1':p1, 'Param_2':p2, 'Param_3':p3, 'Accuracy':m_acc, 'Precision':m_pre, 'Recall':m_rec, 'F1_Score':m_f1s, 'Time':t_ela}\n",
    "    return ret, prc, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Artificial neural network cross validation ===\n",
    "def qsar_ann_cv(X, y, nspl, nsed, p1, p2, p3):\n",
    "\n",
    "    # === Accumulate accuracy ===\n",
    "    acc, pre, rec, f1s, auc = (np.zeros(nspl*nsed) for i in range(5))\n",
    "    i = 0\n",
    "\n",
    "    # === K-fold cross validation ===\n",
    "    t_sta = time.perf_counter()\n",
    "    kfold = KFold(n_splits=nspl, shuffle=True, random_state=42)\n",
    "    for rs in range(nsed):\n",
    "        tf.keras.utils.set_random_seed(rs)\n",
    "        for trn_i, tst_i in kfold.split(X):\n",
    "    \n",
    "            # === Split data ===\n",
    "            X_trn, X_tst = X[trn_i], X[tst_i]\n",
    "            y_trn, y_tst = y[trn_i], y[tst_i]\n",
    "\n",
    "            # === Create and train model ===\n",
    "            model = create_ann(p2,np.shape(X)[1])\n",
    "            model.fit(X_trn, y_trn, epochs=p1, verbose=0)\n",
    "\n",
    "            # === Make predictions ===\n",
    "            yh_tst = model.predict(X_tst)\n",
    "            \n",
    "            auc[i] = met.roc_auc_score(y_tst,yh_tst,average='micro')\n",
    "            prc = met.precision_recall_curve(y_tst,yh_tst)\n",
    "            roc = met.roc_curve(y_tst,yh_tst)\n",
    "            \n",
    "            yh_tst = (yh_tst >= 0.5).astype(int)\n",
    "        \n",
    "            acc[i] = met.accuracy_score(y_tst,yh_tst)\n",
    "            pre[i] = met.precision_score(y_tst,yh_tst)\n",
    "            rec[i] = met.recall_score(y_tst,yh_tst)\n",
    "            f1s[i] = met.f1_score(y_tst,yh_tst)\n",
    "            auc[i] = met.roc_auc_score(y_tst,yh_tst)\n",
    "\n",
    "            i += 1\n",
    "        print(f'Completed seed {rs}.')\n",
    "\n",
    "    t_end = time.perf_counter()\n",
    "    t_ela = t_end-t_sta\n",
    "    \n",
    "    m_acc = np.mean(acc)\n",
    "    m_pre = np.mean(pre)\n",
    "    m_rec = np.mean(rec)\n",
    "    m_f1s = np.mean(f1s)\n",
    "    m_auc = np.mean(auc)\n",
    "    \n",
    "    ret = {'Model':'ANN', 'Param_1':p1, 'Param_2':p2, 'Param_3':p3, 'Accuracy':m_acc, 'Precision':m_pre, 'Recall':m_rec, 'F1_Score':m_f1s, 'ROC_AUC':m_auc, 'Time':t_ela}\n",
    "    return ret,prc,roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data processing dictionary ===\n",
    "# I: Imputed\n",
    "# S: Scaled\n",
    "# O: Oversampled\n",
    "# R: Reduced\n",
    "def get_X(i):\n",
    "    if i == 0:\n",
    "        return Xs, 'S'\n",
    "    if i == 1:\n",
    "        return Xis, 'IS'\n",
    "    if i == 2:\n",
    "        return Xsr, 'SR'\n",
    "    if i == 3:\n",
    "        return Xisr, 'ISR'\n",
    "\n",
    "# === Initialize metrics dataframe ===\n",
    "metrics = pd.DataFrame(columns=['Model', 'Param_1', 'Param_2', 'Param_3', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'Time', 'Data'])\n",
    "prc_m = pd.DataFrame(columns=['P', 'R', 'M'])\n",
    "roc_m = pd.DataFrame(columns=['F', 'T', 'M'])\n",
    "\n",
    "# === PRC and ROC processing function ===\n",
    "def prc_roc(prc, roc, lab):\n",
    "    prc2 = pd.DataFrame(prc[0:2]).transpose()\n",
    "    prc2.columns = ['P', 'R']\n",
    "    prc2['M'] = [lab]*len(prc[0])\n",
    "    roc2 = pd.DataFrame(roc[0:2]).transpose()\n",
    "    roc2.columns = ['F', 'T']\n",
    "    roc2['M'] = [lab]*len(roc[0])\n",
    "    return prc2, roc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afb692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define parameter loops ===\n",
    "def oneparam(X, y, dt, m, p1l, metrics, prc_m, roc_m):\n",
    "    # === Get length of parameter 1 list ===\n",
    "    ll1 = len(p1l)\n",
    "    for i in range(ll1):\n",
    "        # === Get outputs from cross validation ===\n",
    "        os, prc, roc = qsar_cv(X, y, 100, m, p1l[i], 0, 0)\n",
    "        os['Data']=dt\n",
    "        os = pd.DataFrame(os, index=[i])\n",
    "        # === Append to metrics records ===\n",
    "        metrics = pd.concat([metrics, os], axis=0, ignore_index=True)\n",
    "        prc2, roc2 = prc_roc(prc, roc, m+'.'+dt+'.'+str(p1l[i]))\n",
    "        prc_m = pd.concat([prc_m, prc2], axis=0, ignore_index=True)\n",
    "        roc_m = pd.concat([roc_m, roc2], axis=0, ignore_index=True)\n",
    "        print(p1l[i])\n",
    "    return metrics, prc_m, roc_m\n",
    "\n",
    "def twoparam(X, y, dt, m, p1l, p2l, metrics, prc_m, roc_m):\n",
    "    ll1 = len(p1l)\n",
    "    ll2 = len(p2l)\n",
    "    for i in range(ll1):\n",
    "        for j in range(ll2):\n",
    "            os, prc, roc = qsar_cv(X, y, 100, m, p1l[i], p2l[j], 0)\n",
    "            os['Data']=dt\n",
    "            os = pd.DataFrame(os, index=[i*ll2+j])\n",
    "            metrics = pd.concat([metrics, os], axis=0, ignore_index=True)\n",
    "            prc2, roc2 = prc_roc(prc, roc, m+'.'+dt+'.'+str(p1l[i])+'.'+str(p2l[j]))\n",
    "            prc_m = pd.concat([prc_m, prc2], axis=0, ignore_index=True)\n",
    "            roc_m = pd.concat([roc_m, roc2], axis=0, ignore_index=True)\n",
    "            print(str(p1l[i])+', '+str(p2l[j]))\n",
    "    return metrics, prc_m, roc_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b33da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Logistic regression ===\n",
    "p1l = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky']\n",
    "for a in range(4):\n",
    "    X, dt = get_X(a)\n",
    "    metrics, prc_m, roc_m = oneparam(X, y, dt, 'LR', p1l, metrics, prc_m, roc_m)\n",
    "\n",
    "print('LR complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Support vector machines ===\n",
    "p1l = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "ll1 = len(p1l)\n",
    "for a in range(4):\n",
    "    X, dt = get_X(a)\n",
    "    metrics, prc_m, roc_m = oneparam(X, y, dt, 'SVM', p1l, metrics, prc_m, roc_m)\n",
    "\n",
    "print('SVM complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8482393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Random forest ===\n",
    "p1l = [50, 100, 300]\n",
    "p2l = [5, 10]\n",
    "for a in range(4):\n",
    "    X, dt = get_X(a)\n",
    "    metrics, prc_m, roc_m = twoparam(X, y, dt, 'RF', p1l, p2l, metrics, prc_m, roc_m)\n",
    "\n",
    "print('RF complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Gradient boosted trees ===\n",
    "p1l = [3, 5, 10]\n",
    "for a in range(4):\n",
    "    X, dt = get_X(a)\n",
    "    metrics, prc_m, roc_m = oneparam(X, y, dt, 'GBT', p1l, metrics, prc_m, roc_m)\n",
    "\n",
    "print('GBT complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830df4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Artificial neural networks ===\n",
    "p1l = [100, 200, 400]\n",
    "ll1 = len(p1l)\n",
    "for i in range(ll1,):\n",
    "    os,prc,roc = qsar_ann_cv(Xs,y,5,10,p1l[i],0.2,0)\n",
    "    os['Data']='Scaled'\n",
    "    os = pd.DataFrame(os,index=[i])\n",
    "    metrics = pd.concat([metrics,os],axis=0,ignore_index=True)\n",
    "    prc2,roc2 = prc_roc(prc,roc,'ANN.S.0.2.'+str(p1l[i]))\n",
    "    prc_m = pd.concat([prc_m,prc2],axis=0,ignore_index=True)\n",
    "    roc_m = pd.concat([roc_m,roc2],axis=0,ignore_index=True)\n",
    "p2l = [0, 0.1, 0.2]\n",
    "ll2 = len(p2l)\n",
    "for i in range(ll2):\n",
    "    os,prc,roc = qsar_ann_cv(Xs,y,5,10,400,p2l[i],0)\n",
    "    os['Data']='Scaled'\n",
    "    os = pd.DataFrame(os,index=[i])\n",
    "    metrics = pd.concat([metrics,os],axis=0,ignore_index=True)\n",
    "    prc2,roc2 = prc_roc(prc,roc,'ANN.R.'+str(p2l[i])+'.400')\n",
    "    prc_m = pd.concat([prc_m,prc2],axis=0,ignore_index=True)\n",
    "    roc_m = pd.concat([roc_m,roc2],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Write metrics to file ===\n",
    "metrics.to_csv('RS_MULTI_MET.csv', mode='a', index=False, header=False)\n",
    "prc_m.to_csv('RS_MULTI_PRC.csv', mode='a', index=False, header=False)\n",
    "roc_m.to_csv('RS_MULTI_ROC.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9719b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Explainable AI via SHAP ===\n",
    "shap.initjs()\n",
    "Xd = DMatrix(Xs_pd, label=y)\n",
    "shap_model = train({'eta':1, 'max_depth':3, 'base_score':0, 'lambda':0}, Xd, 1)\n",
    "shap_pred = shap_model.predict(Xd, output_margin=True)\n",
    "explainer = shap.TreeExplainer(shap_model)\n",
    "explanation = explainer(Xd)\n",
    "shap_values = explanation.values\n",
    "\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, Xs_pd, max_display=10, show=False)\n",
    "plt.savefig('temp_out/shap_summary.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
    "plt.close()\n",
    "\n",
    "#shap.force_plot(explainer.expected_value, shap_values, Xs_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8231abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optuna ===\n",
    "def func(trial):\n",
    "    layers = []\n",
    "    n_layers = trial.suggest_int('n_layers', l_min, l_max)\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trail.suggest_int(str(i), n_min, n_max))\n",
    "    clf = Model(hidden_layer_sizes=tuple(layers))\n",
    "    clf.fit(X_trn, y_trn)\n",
    "    return clf.score(X_tst, y_tst)\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(func, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
