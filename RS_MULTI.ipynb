{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d9c11b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Graph' from 'networkx' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chem\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSaltRemover\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SaltRemover\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmordred\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Calculator, descriptors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# === Impute, oversample, and undersample ===\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\James\\Lib\\site-packages\\mordred\\descriptors\\__init__.py:42\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(names)\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(values)\n\u001b[1;32m---> 42\u001b[0m _import_all_descriptors()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\James\\Lib\\site-packages\\mordred\\descriptors\\__init__.py:31\u001b[0m, in \u001b[0;36m_import_all_descriptors\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ext \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m mdl \u001b[38;5;241m=\u001b[39m import_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name, __package__)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m mdl\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m is_descriptor_class(v)):\n\u001b[0;32m     34\u001b[0m     names\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\James\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\James\\Lib\\site-packages\\mordred\\BaryszMatrix.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Graph, floyd_warshall_numpy\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Descriptor\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_atomic_property\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AtomicProperty, get_properties\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Graph' from 'networkx' (unknown location)"
     ]
    }
   ],
   "source": [
    "# === Basic libraries ===\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Feature generation ===\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.SaltRemover import SaltRemover\n",
    "from mordred import Calculator, descriptors\n",
    "\n",
    "# === Impute, oversample, and undersample ===\n",
    "from sklearn.impute import SimpleImputer\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# === Dimensionality reduction ===\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# === Classifiers ===\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier, DMatrix, train\n",
    "\n",
    "# === Metrics and cross-validation ====\n",
    "import logging\n",
    "from sklearn import metrics as met\n",
    "from sklearn.model_selection import cross_val_predict, LeaveOneOut, KFold\n",
    "import shap\n",
    "\n",
    "# === Neural networks from Tensorflow ===\n",
    "import tensorflow as tf\n",
    "from keras import Model, layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.data import Iterator\n",
    "\n",
    "# === Neural networks from Pytorch Lightning ===\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import lightning as L\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "# === Optimization ===\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb88f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Read in files with X,y ===\n",
    "RS_XY = pd.read_csv('rsxy_v1.csv')\n",
    "clist = list(RS_XY['SMILES'])\n",
    "y = np.array(RS_XY['Sens'])\n",
    "y = np.reshape(y,(-1,1))\n",
    "X = np.zeros(shape=(len(clist),1826))\n",
    "\n",
    "# === Calculate descriptors ===\n",
    "calc = Calculator(descriptors)\n",
    "dlist = list(calc._name_dict.keys())\n",
    "remover = SaltRemover()\n",
    "for i in range(len(clist)):\n",
    "    mol = Chem.MolFromSmiles(clist[i])\n",
    "    #mol = remover.StripMol(mol)\n",
    "    X[i,:] = calc(mol)\n",
    "sh1 = np.shape(X)\n",
    "print(f'Shape | raw: {sh1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Set up oversampler ===\n",
    "oversample = False\n",
    "ovs = SMOTE()\n",
    "\n",
    "# === Impute missing values ===\n",
    "imp_med = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "Xi = imp_med.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11caa4b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Filter data and scale ===\n",
    "X0 = X\n",
    "X = X[:,~np.any(np.isnan(X), axis=0)]\n",
    "X = X[:, np.var(X, axis=0) != 0]\n",
    "Xs = pp.MinMaxScaler().fit_transform(X)\n",
    "sh2 = np.shape(Xs)\n",
    "print(f'Shape | filtered/scaled: {sh2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Filter data and scale ===\n",
    "X_pd = pd.DataFrame(X0, columns=dlist)\n",
    "X_pd.to_csv('temp_out/X.csv', header=True)\n",
    "Xs_pd = X_pd.dropna(axis=1)\n",
    "Xs_pd = Xs_pd.loc[:, Xs_pd.var()!=0]\n",
    "Xs_pd.to_csv('temp_out/Xs.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46b0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Filter data and scale ===\n",
    "Xi = Xi[:,~np.any(np.isnan(Xi), axis=0)]\n",
    "Xi = Xi[:, np.var(Xi, axis=0) != 0]\n",
    "Xis = pp.MinMaxScaler().fit_transform(Xi)\n",
    "sh3 = np.shape(Xis)\n",
    "print(f'Shape | filtered/scaled: {sh3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17952878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Heatmap of feature correlation ===\n",
    "Xs_pd = pd.DataFrame(Xs)\n",
    "hm1 = sns.heatmap(Xs_pd.corr())\n",
    "hm1.figure.savefig('temp_out/Xs.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc607a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Imputed features ===\n",
    "Xis_pd = pd.DataFrame(Xis)\n",
    "hm1 = sns.heatmap(Xis_pd.corr())\n",
    "hm1.figure.savefig('temp_out/Xis.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401449b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Conduct PCA and display updated heatmap ===\n",
    "pca = PCA(n_components=100,random_state=np.random.seed(0))\n",
    "Xsr = pca.fit_transform(Xs_pd)\n",
    "Xsr_pd = pd.DataFrame(Xsr)\n",
    "hm2 = sns.heatmap(Xsr_pd.corr())\n",
    "hm2.figure.savefig('temp_out/Xsr.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PCA for imputed features ===\n",
    "Xisr = pca.fit_transform(Xis_pd)\n",
    "Xisr_pd = pd.DataFrame(Xisr)\n",
    "hm2 = sns.heatmap(Xisr_pd.corr())\n",
    "hm2.figure.savefig('temp_out/Xisr.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f3821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reduce verbosity ===\n",
    "log = logging.getLogger('lightning_fabric')\n",
    "log.propagate = False\n",
    "log.setLevel(logging.ERROR)\n",
    "L.utilities.distributed.log.setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "logging.getLogger('lightning').setLevel(0)\n",
    "\n",
    "# === Convert to torch tensors ===\n",
    "def to_tensor(X_trn, y_trn, X_tst, y_tst):\n",
    "    X_trn = torch.tensor(X_trn, dtype=torch.float32)\n",
    "    y_trn = torch.tensor(y_trn, dtype=torch.int32).reshape(-1, 1)\n",
    "    X_tst = torch.tensor(X_tst, dtype=torch.float32)\n",
    "    y_tst = torch.tensor(y_tst, dtype=torch.int32).reshape(-1, 1)\n",
    "    return X_trn, y_trn, X_tst, y_tst\n",
    "\n",
    "# === Lightning module for MLP ===\n",
    "class rscls_mlp(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, xsh: int, l1n: int, l2n: int):\n",
    "        super().__init__()\n",
    "        self.dr = nn.Dropout(p=0.2)\n",
    "        self.l1 = nn.Linear(xsh, l1n)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(l1n, l2n)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.out = nn.Linear(l2n, 1)\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dr(x)\n",
    "        x = self.a1(self.l1(x))\n",
    "        x = self.a2(self.l2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "        loss = loss(logits, y.float())\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "        loss = loss(logits, y.float())\n",
    "        self.log('val_loss', loss)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        #avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        #tensorboard_logs = {'val_loss': avg_loss}\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()\n",
    "        #return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    '''\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = mse_loss(logits, y)\n",
    "        correct = torch.sum(logits == y.data)\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        logs = {'test_loss': avg_loss}      \n",
    "        return {'avg_test_loss': avg_loss, 'log': logs, 'progress_bar': logs }\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TensorDataset(torch.tensor(train_features.values).float(), torch.tensor(train_targets[['cnt']].values).float())\n",
    "        train_loader = DataLoader(dataset = train_dataset, batch_size = 16)\n",
    "        return train_loader\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        validation_dataset = TensorDataset(torch.tensor(validation_features.values).float(), torch.tensor(validation_targets[['cnt']].values).float())\n",
    "        validation_loader = DataLoader(dataset = validation_dataset, batch_size = 16)\n",
    "        return validation_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TensorDataset(torch.tensor(test_features.values).float(), torch.tensor(test_targets[['cnt']].values).float())\n",
    "        test_loader = DataLoader(dataset = test_dataset, batch_size = 16)\n",
    "        return test_loader\n",
    "    '''\n",
    "\n",
    "# === Lightning module for KD ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cross validation function ===\n",
    "def qsar_cv(X, y, n, m, p1, p2, p3, nspl=5, oversample=False):\n",
    "    xsh = np.shape(X)[1]\n",
    "    # === Initialize metrtics and iterator ===\n",
    "    acc, pre, rec, f1s = (np.zeros(nspl*n) for i in range(4))\n",
    "    i = 0\n",
    "    # === Start stopwatch ===\n",
    "    t_sta = time.perf_counter()\n",
    "    \n",
    "    # === Loop over n random seeds ===\n",
    "    ytst_cv = []\n",
    "    prob_cv = []\n",
    "    for rs in range(n):\n",
    "        # === Initialize k-fold cross validation ===\n",
    "        kfold = KFold(n_splits=nspl, shuffle=True, random_state=np.random.seed(rs))\n",
    "        # === Set model ===\n",
    "        if m == 'LR':\n",
    "            model = LogisticRegression(solver=p1,random_state=np.random.seed(rs),max_iter=200)\n",
    "        if m == 'SVM':\n",
    "            model = svm.SVC(kernel=p1,random_state=np.random.seed(rs),probability=True)\n",
    "        if m == 'RF':\n",
    "            model = RandomForestClassifier(n_estimators=p1,max_depth=p2,random_state=np.random.seed(rs))\n",
    "        if m == 'GBT':\n",
    "            model = XGBClassifier(max_depth=p1,seed=rs)\n",
    "        if m == 'MLP':\n",
    "            # === Use GPU if available, else use CPU ===\n",
    "            #device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "            seed_everything(rs)\n",
    "            model = rscls_mlp(xsh, p1, p2)#.to(device)\n",
    "        \n",
    "        for trn_i, tst_i in kfold.split(X):\n",
    "            # === Split data ===\n",
    "            X_trn, X_tst = X[trn_i], X[tst_i]\n",
    "            y_trn, y_tst = y[trn_i], y[tst_i]\n",
    "            # === Oversample ===\n",
    "            if oversample == True:\n",
    "                X_trn, y_trn = ovs.fit_resample(X_trn, y_trn)\n",
    "            \n",
    "            # === Fit and predict ===\n",
    "            if m == 'MLP':\n",
    "                # === Dataloaders ===\n",
    "                X_trn, y_trn, X_tst, y_tst = to_tensor(X_trn, y_trn, X_tst, y_tst)\n",
    "                trainset = torch.utils.data.TensorDataset(X_trn, y_trn)\n",
    "                testset = torch.utils.data.TensorDataset(X_tst, y_tst)\n",
    "                trn_load = DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "                tst_load = DataLoader(testset, batch_size=16, shuffle=False)\n",
    "                trainer = L.Trainer(max_epochs=10, deterministic=True, enable_model_summary=False, enable_progress_bar=False)\n",
    "                trainer.fit(model, trn_load, tst_load)\n",
    "                prob = trainer.predict(model, X_tst)\n",
    "                prob = torch.cat(prob).numpy()\n",
    "                pred = (prob >= 0.5).astype(int)\n",
    "            elif m == 'KD':\n",
    "                prob = []\n",
    "                pred = []\n",
    "            else:\n",
    "                model.fit(X_trn, y_trn.ravel())\n",
    "                prob = model.predict_proba(X_tst)[:,1]\n",
    "                pred = model.predict(X_tst)\n",
    "            \n",
    "            # === Append for performance curves ===\n",
    "            ytst_cv.append(y_tst)\n",
    "            prob_cv.append(prob)\n",
    "            \n",
    "            # === Calculate metrics ===\n",
    "            acc[i] = met.accuracy_score(y_tst,pred)\n",
    "            pre[i] = met.precision_score(y_tst,pred)\n",
    "            rec[i] = met.recall_score(y_tst,pred)\n",
    "            f1s[i] = met.f1_score(y_tst,pred)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "    # === Stop stopwatch ===\n",
    "    t_end = time.perf_counter()\n",
    "    t_ela = t_end-t_sta\n",
    "\n",
    "    # === Average metrics ===\n",
    "    m_acc = np.mean(acc)\n",
    "    m_pre = np.mean(pre)\n",
    "    m_rec = np.mean(rec)\n",
    "    m_f1s = np.mean(f1s)\n",
    "    ytst_cv = np.concatenate(ytst_cv)\n",
    "    prob_cv = np.concatenate(prob_cv)\n",
    "    prc = met.precision_recall_curve(ytst_cv, prob_cv)\n",
    "    roc = met.roc_curve(ytst_cv, prob_cv)\n",
    "    \n",
    "    ret = {'Model':m, 'Param_1':p1, 'Param_2':p2, 'Param_3':p3, 'Accuracy':m_acc, 'Precision':m_pre, 'Recall':m_rec, 'F1_Score':m_f1s, 'Time':t_ela}\n",
    "    return ret, prc, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Retrieve processed data ===\n",
    "# I: Imputed\n",
    "# S: Scaled\n",
    "# O: Oversampled\n",
    "# R: Reduced\n",
    "def get_X(i):\n",
    "    if i == 0:\n",
    "        return Xs, 'S'\n",
    "    if i == 1:\n",
    "        return Xis, 'IS'\n",
    "    if i == 2:\n",
    "        return Xsr, 'SR'\n",
    "    if i == 3:\n",
    "        return Xisr, 'ISR'\n",
    "\n",
    "# === Initialize metrics dataframe ===\n",
    "metrics = pd.DataFrame(columns=['Model', 'Param_1', 'Param_2', 'Param_3', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'Time', 'Data'])\n",
    "prc_m = pd.DataFrame(columns=['P', 'R', 'M'])\n",
    "roc_m = pd.DataFrame(columns=['F', 'T', 'M'])\n",
    "\n",
    "# === PRC and ROC processing function ===\n",
    "def prc_roc(prc, roc, lab):\n",
    "    prc2 = pd.DataFrame(prc[0:2]).transpose()\n",
    "    prc2.columns = ['P', 'R']\n",
    "    prc2['M'] = [lab]*len(prc[0])\n",
    "    roc2 = pd.DataFrame(roc[0:2]).transpose()\n",
    "    roc2.columns = ['F', 'T']\n",
    "    roc2['M'] = [lab]*len(roc[0])\n",
    "    return prc2, roc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b022c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define parameter loops ===\n",
    "def oneparam(X, y, dt, m, p1l, metrics, prc_m, roc_m, oversample):\n",
    "    if oversample == True:\n",
    "        dt = dt+'O'\n",
    "    # === Get length of parameter 1 list ===\n",
    "    ll1 = len(p1l)\n",
    "    for i in range(ll1):\n",
    "        # === Get outputs from cross validation ===\n",
    "        os, prc, roc = qsar_cv(X, y, 50, m, p1l[i], 0, 0, oversample=oversample)\n",
    "        os['Data']=dt\n",
    "        os = pd.DataFrame(os, index=[i])\n",
    "        # === Append to metrics records ===\n",
    "        metrics = pd.concat([metrics, os], axis=0, ignore_index=True)\n",
    "        prc2, roc2 = prc_roc(prc, roc, m+'.'+dt+'.'+str(p1l[i]))\n",
    "        prc_m = pd.concat([prc_m, prc2], axis=0, ignore_index=True)\n",
    "        roc_m = pd.concat([roc_m, roc2], axis=0, ignore_index=True)\n",
    "        print(p1l[i])\n",
    "    return metrics, prc_m, roc_m\n",
    "\n",
    "def twoparam(X, y, dt, m, p1l, p2l, metrics, prc_m, roc_m, oversample):\n",
    "    if oversample == True:\n",
    "        dt = dt+'O'\n",
    "    ll1 = len(p1l)\n",
    "    ll2 = len(p2l)\n",
    "    for i in range(ll1):\n",
    "        for j in range(ll2):\n",
    "            os, prc, roc = qsar_cv(X, y, 50, m, p1l[i], p2l[j], 0, oversample=oversample)\n",
    "            os['Data']=dt\n",
    "            os = pd.DataFrame(os, index=[i*ll2+j])\n",
    "            metrics = pd.concat([metrics, os], axis=0, ignore_index=True)\n",
    "            prc2, roc2 = prc_roc(prc, roc, m+'.'+dt+'.'+str(p1l[i])+'.'+str(p2l[j]))\n",
    "            prc_m = pd.concat([prc_m, prc2], axis=0, ignore_index=True)\n",
    "            roc_m = pd.concat([roc_m, roc2], axis=0, ignore_index=True)\n",
    "            print(str(p1l[i])+', '+str(p2l[j]))\n",
    "    return metrics, prc_m, roc_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b33da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Logistic regression ===\n",
    "p1l = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky']\n",
    "for a in range(4):\n",
    "    X, dt = get_X(a)\n",
    "    metrics, prc_m, roc_m = oneparam(X, y, dt, 'LR', p1l, metrics, prc_m, roc_m, oversample=False)\n",
    "    metrics, prc_m, roc_m = oneparam(X, y, dt, 'LR', p1l, metrics, prc_m, roc_m, oversample=True)\n",
    "\n",
    "print('LR complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Support vector machines ===\n",
    "p1l = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "ll1 = len(p1l)\n",
    "for a in range(4):\n",
    "    X, dt = get_X(a)\n",
    "    metrics, prc_m, roc_m = oneparam(X, y, dt, 'SVM', p1l, metrics, prc_m, roc_m, oversample=False)\n",
    "    metrics, prc_m, roc_m = oneparam(X, y, dt, 'SVM', p1l, metrics, prc_m, roc_m, oversample=True)\n",
    "\n",
    "print('SVM complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8482393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Random forest ===\n",
    "p1l = [50, 100, 300]\n",
    "p2l = [5, 10]\n",
    "for a in range(4):\n",
    "    X, dt = get_X(a)\n",
    "    metrics, prc_m, roc_m = twoparam(X, y, dt, 'RF', p1l, p2l, metrics, prc_m, roc_m, oversample=False)\n",
    "    metrics, prc_m, roc_m = twoparam(X, y, dt, 'RF', p1l, p2l, metrics, prc_m, roc_m, oversample=True)\n",
    "\n",
    "print('RF complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Gradient boosted trees ===\n",
    "p1l = [3, 5, 10]\n",
    "for a in range(4):\n",
    "    X, dt = get_X(a)\n",
    "    metrics, prc_m, roc_m = oneparam(X, y, dt, 'GBT', p1l, metrics, prc_m, roc_m, oversample=False)\n",
    "    metrics, prc_m, roc_m = oneparam(X, y, dt, 'GBT', p1l, metrics, prc_m, roc_m, oversample=True)\n",
    "\n",
    "print('GBT complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Write metrics to file ===\n",
    "metrics.to_csv('RS_MULTI_MET.csv', mode='a', index=False, header=False)\n",
    "prc_m.to_csv('RS_MULTI_PRC.csv', mode='a', index=False, header=False)\n",
    "roc_m.to_csv('RS_MULTI_ROC.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5a29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Multilayer perceptron ===\n",
    "p1l = [100, 50]\n",
    "p2l = [25, 10]\n",
    "for a in range(4):\n",
    "    X, dt = get_X(a)\n",
    "    metrics, prc_m, roc_m = twoparam(X, y, dt, 'MLP', p1l, p2l, metrics, prc_m, roc_m, oversample=False)\n",
    "    metrics, prc_m, roc_m = twoparam(X, y, dt, 'MLP', p1l, p2l, metrics, prc_m, roc_m, oversample=True)\n",
    "    \n",
    "print('MLP complete')\n",
    "\n",
    "# === Write metrics to file ===\n",
    "metrics.to_csv('RS_MULTI_MET.csv', mode='a', index=False, header=False)\n",
    "prc_m.to_csv('RS_MLP_PRC.csv', mode='a', index=False, header=False)\n",
    "roc_m.to_csv('RS_MLP_ROC.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82858f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Explainable AI via SHAP ===\n",
    "shap.initjs()\n",
    "Xd = DMatrix(Xs_pd, label=y)\n",
    "shap_model = train({'eta':1, 'max_depth':3, 'base_score':0, 'lambda':0}, Xd, 1)\n",
    "shap_pred = shap_model.predict(Xd, output_margin=True)\n",
    "explainer = shap.TreeExplainer(shap_model)\n",
    "explanation = explainer(Xd)\n",
    "shap_values = explanation.values\n",
    "\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, Xs_pd, max_display=10, show=False)\n",
    "plt.savefig('temp_out/shap_summary.tiff',dpi=300,pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
    "plt.close()\n",
    "\n",
    "#shap.force_plot(explainer.expected_value, shap_values, Xs_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8231abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optuna ===\n",
    "def func(trial):\n",
    "    layers = []\n",
    "    n_layers = trial.suggest_int('n_layers', l_min, l_max)\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trail.suggest_int(str(i), n_min, n_max))\n",
    "    clf = Model(hidden_layer_sizes=tuple(layers))\n",
    "    clf.fit(X_trn, y_trn)\n",
    "    return clf.score(X_tst, y_tst)\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(func, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
