{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eac6565-82d2-4efa-b9d9-aed747c75284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 980 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Basic libraries ===\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "# === Neural networks from Pytorch Lightning ===\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import lightning as L\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "# === Optimization ===\n",
    "#import optuna\n",
    "\n",
    "# === Check for GPU ===\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7b5ddc-7285-4967-8221-66569809ef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape | filtered/scaled: (158, 656)\n"
     ]
    }
   ],
   "source": [
    "# === Set project ===\n",
    "proj = 'LSI'\n",
    "proj = 'RS'\n",
    "\n",
    "# === Read Xy ===\n",
    "X_pd = pd.read_csv(proj+'_X.csv', index_col=0)\n",
    "X = X_pd.to_numpy()\n",
    "y = np.genfromtxt(proj+'_y.csv', delimiter=',')\n",
    "\n",
    "# === Filter data and scale ===\n",
    "Xs_pd = X_pd.dropna(axis=1)\n",
    "Xs_pd = Xs_pd.loc[:, Xs_pd.var()!=0]\n",
    "Xs_pd.to_csv('temp_out/'+proj+'_Xs.csv', header=True)\n",
    "X = X[:,~np.any(np.isnan(X), axis=0)]\n",
    "X = X[:, np.var(X, axis=0) != 0]\n",
    "Xs = pp.MinMaxScaler().fit_transform(X)\n",
    "sh1 = np.shape(Xs)\n",
    "print(f'Shape | filtered/scaled: {sh1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a3311-7a6a-472a-9d7a-926a8c8d4bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout: float, output_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        input_dim: int = 656\n",
    "        for output_dim in output_dims:\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = output_dim\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, CLASSES))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.layers(data)\n",
    "        return logits\n",
    "        #return F.log_softmax(logits, dim=1)\n",
    "\n",
    "class LightningNet(pl.LightningModule):\n",
    "    def __init__(self, dropout: float, output_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.model = Net(dropout, output_dims)\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(data.view(-1, 656))\n",
    "\n",
    "    def training_step(self, batch: List[torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        return F.binary_cross_entropy_with_logits(output, target)\n",
    "\n",
    "    def validation_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        accuracy = pred.eq(target.view_as(pred)).float().mean()\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "        self.log(\"hp_metric\", accuracy, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self) -> optim.Optimizer:\n",
    "        return optim.Adam(self.model.parameters())\n",
    "\n",
    "\n",
    "class qsardm(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        self.X_trn, self.X_val = random_split(X, [120, 30])\n",
    "        self.X_tst = self.X_val\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.X_trn, batch_size=self.batch_size, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.X_val, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.X_tst, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    output_dims = [\n",
    "        trial.suggest_int(\"n_units_l{}\".format(i), 4, 128, log=True) for i in range(n_layers)\n",
    "    ]\n",
    "\n",
    "    model = LightningNet(dropout, output_dims)\n",
    "    datamodule = qsardm(data_dir=DIR, batch_size=BATCHSIZE)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,\n",
    "        limit_val_batches=PERCENT_VALID_EXAMPLES,\n",
    "        enable_checkpointing=False,\n",
    "        max_epochs=EPOCHS,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")],\n",
    "    )\n",
    "    hyperparameters = dict(n_layers=n_layers, dropout=dropout, output_dims=output_dims)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_acc\"].item()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Lightning example.\")\n",
    "    parser.add_argument(\n",
    "        \"--pruning\",\n",
    "        \"-p\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Activate the pruning feature. `MedianPruner` stops unpromising \"\n",
    "        \"trials at the early stages of training.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner() if args.pruning else optuna.pruners.NopPruner()\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb0684-ba6c-4c31-b48d-316a43e61a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optuna ===\n",
    "def func(trial):\n",
    "    layers = []\n",
    "    n_layers = trial.suggest_int('n_layers', l_min, l_max)\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trail.suggest_int(str(i), n_min, n_max))\n",
    "    clf = Model(hidden_layer_sizes=tuple(layers))\n",
    "    clf.fit(X_trn, y_trn)\n",
    "    return clf.score(X_tst, y_tst)\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(func, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
